{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb83a306fd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x=x.astype('float32')\n",
    "    x/= 255.0\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from tflearn.data_utils import to_categorical\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return to_categorical(x, 10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, *image_shape], name = 'x')\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #input_depth = x_tensor.get_shape().as_list()[3]\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], conv_num_outputs], mean=0.0, stddev=0.01)) # (height, width, input_depth, output_depth)\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, filter_bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.get_shape().as_list()[1]*x_tensor.get_shape().as_list()[2]*x_tensor.get_shape().as_list()[3]])\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.01)) \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    #How does tf know to multiple weights to the dimension 2 of\n",
    "    #x_tensor (the first dimention is the batch size)\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, weight), bias))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.01)) \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    #How does tf know to multiple weights to the dimension 2 of\n",
    "    #x_tensor (the first dimention is the batch size)\n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    layer = conv2d_maxpool(x, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    layer = conv2d_maxpool(layer, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    layer = conv2d_maxpool(layer, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    #layer = tf.nn.dropout(layer, keep_prob=keep_prob)    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    layer = flatten(layer)   \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    layer = fully_conn(layer, 512)\n",
    "    layer = fully_conn(layer, 512)  \n",
    "    layer = tf.nn.dropout(layer, keep_prob=keep_prob)    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    layer = output(layer, 10)    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #x = neural_net_image_input([feature_batch.shape[1], feature_batch.shape[2], feature_batch.shape[3]])\n",
    "    #y = neural_net_label_input(label_batch.shape[1])\n",
    "    #keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "    #print(feature_batch.shape)\n",
    "    #print(label_batch.shape)\n",
    "\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels,  keep_prob: 1.0})\n",
    "    print('loss', loss, 'accuracy', valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.24837 accuracy 0.1824\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 2.20412 accuracy 0.2408\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 2.21051 accuracy 0.2572\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 2.15497 accuracy 0.292\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.96845 accuracy 0.3264\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.85394 accuracy 0.3558\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.81292 accuracy 0.3814\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.74132 accuracy 0.384\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.69922 accuracy 0.411\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.67753 accuracy 0.4204\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.67121 accuracy 0.4252\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 1.61658 accuracy 0.4344\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 1.60812 accuracy 0.4308\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 1.51199 accuracy 0.44\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 1.51656 accuracy 0.4338\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 1.4622 accuracy 0.454\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 1.44379 accuracy 0.4484\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 1.38415 accuracy 0.4608\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 1.34063 accuracy 0.4646\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 1.28484 accuracy 0.4684\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 1.2433 accuracy 0.4708\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 1.18287 accuracy 0.4824\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 1.14924 accuracy 0.473\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 1.11176 accuracy 0.4836\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 1.07792 accuracy 0.488\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 1.00522 accuracy 0.4872\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.99578 accuracy 0.4932\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.974202 accuracy 0.4916\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.923288 accuracy 0.4892\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.95085 accuracy 0.4924\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.88669 accuracy 0.4978\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.862024 accuracy 0.5006\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.822437 accuracy 0.499\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.768685 accuracy 0.5006\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.753587 accuracy 0.4948\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.766574 accuracy 0.4746\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.69822 accuracy 0.4864\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.689458 accuracy 0.4942\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.627022 accuracy 0.5034\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.582278 accuracy 0.4976\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.552364 accuracy 0.4946\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.515297 accuracy 0.4972\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.526667 accuracy 0.4878\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.511924 accuracy 0.4986\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.538598 accuracy 0.484\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.549777 accuracy 0.4922\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.528646 accuracy 0.4844\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.473743 accuracy 0.4854\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.500225 accuracy 0.4906\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.505578 accuracy 0.4862\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.48489 accuracy 0.4944\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.459165 accuracy 0.4898\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.407361 accuracy 0.4942\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.373956 accuracy 0.4914\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.38788 accuracy 0.4912\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.379831 accuracy 0.4796\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.356481 accuracy 0.4816\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.382381 accuracy 0.4708\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.379542 accuracy 0.4702\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.370895 accuracy 0.4752\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.379432 accuracy 0.4862\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.385561 accuracy 0.4854\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.353893 accuracy 0.48\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.370513 accuracy 0.489\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.32856 accuracy 0.484\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.293086 accuracy 0.4996\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.287628 accuracy 0.492\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.282152 accuracy 0.4958\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.25548 accuracy 0.4958\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.223666 accuracy 0.5036\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.227112 accuracy 0.5038\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.216267 accuracy 0.4948\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.179788 accuracy 0.491\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.172073 accuracy 0.4932\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.171557 accuracy 0.4898\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.147154 accuracy 0.4956\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.156058 accuracy 0.4888\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.158238 accuracy 0.4842\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.186865 accuracy 0.4642\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.190723 accuracy 0.4614\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.15909 accuracy 0.4762\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.144671 accuracy 0.4708\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.163823 accuracy 0.4776\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.122734 accuracy 0.4822\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.0955146 accuracy 0.4876\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.100771 accuracy 0.4892\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.088301 accuracy 0.4866\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.0786276 accuracy 0.4898\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.0768022 accuracy 0.4894\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.0607117 accuracy 0.4828\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.0623182 accuracy 0.4858\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.0588063 accuracy 0.484\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.0673654 accuracy 0.486\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.0606665 accuracy 0.4852\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.0616033 accuracy 0.4858\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.0517282 accuracy 0.4952\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.0569843 accuracy 0.4862\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.0436803 accuracy 0.4926\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.050402 accuracy 0.4906\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.0508837 accuracy 0.4938\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 0.052511 accuracy 0.4912\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 0.0507841 accuracy 0.4982\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 0.0394735 accuracy 0.4916\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 0.0699703 accuracy 0.491\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 0.0664826 accuracy 0.4982\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 0.0825377 accuracy 0.4952\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 0.0830603 accuracy 0.4986\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 0.0741755 accuracy 0.4882\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 0.0721038 accuracy 0.4908\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 0.0610882 accuracy 0.4882\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 0.0448943 accuracy 0.4854\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 0.0460738 accuracy 0.476\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 0.0755297 accuracy 0.4768\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 0.0330935 accuracy 0.4722\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 0.0449822 accuracy 0.484\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 0.0355609 accuracy 0.481\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 0.026522 accuracy 0.4834\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 0.0326046 accuracy 0.471\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 0.0272864 accuracy 0.4822\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 0.0284141 accuracy 0.4852\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 0.0230824 accuracy 0.4936\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 0.0300488 accuracy 0.4888\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 0.0188332 accuracy 0.4864\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 0.0236165 accuracy 0.4876\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 0.0331651 accuracy 0.4814\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 0.0180116 accuracy 0.4932\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 0.0251157 accuracy 0.4856\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 0.027707 accuracy 0.4838\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 0.0153772 accuracy 0.4854\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 0.0290228 accuracy 0.4862\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 0.0170713 accuracy 0.478\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 0.0200846 accuracy 0.4958\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 0.008915 accuracy 0.4992\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 0.00828211 accuracy 0.4866\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 0.0121655 accuracy 0.4878\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 0.0167174 accuracy 0.4914\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 0.0144219 accuracy 0.4818\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 0.032921 accuracy 0.4896\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 0.0187534 accuracy 0.4854\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 0.0225287 accuracy 0.4774\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 0.0128056 accuracy 0.4894\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 0.018879 accuracy 0.493\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 0.014083 accuracy 0.488\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 0.00842395 accuracy 0.4866\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 0.00942811 accuracy 0.4822\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.0146654 accuracy 0.4918\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.0156508 accuracy 0.493\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.0141512 accuracy 0.4914\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.0148547 accuracy 0.4916\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.0149838 accuracy 0.4898\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss 0.00662391 accuracy 0.4864\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss 0.00974847 accuracy 0.495\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss 0.0144594 accuracy 0.4918\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss 0.02703 accuracy 0.4888\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss 0.0174126 accuracy 0.4826\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss 0.00686967 accuracy 0.4898\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss 0.00465595 accuracy 0.4902\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss 0.00426049 accuracy 0.487\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss 0.00536597 accuracy 0.4888\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss 0.0185336 accuracy 0.4904\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss 0.0193443 accuracy 0.4896\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss 0.0172862 accuracy 0.4878\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss 0.0154704 accuracy 0.4896\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss 0.00725208 accuracy 0.4774\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss 0.0240934 accuracy 0.4688\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss 0.0171425 accuracy 0.469\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss 0.0128242 accuracy 0.4772\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss 0.0086189 accuracy 0.4782\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss 0.00696068 accuracy 0.4862\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss 0.00928329 accuracy 0.465\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss 0.0167527 accuracy 0.4724\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss 0.00846835 accuracy 0.4808\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss 0.00932833 accuracy 0.4756\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss 0.00524102 accuracy 0.482\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss 0.00528962 accuracy 0.4916\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss 0.00454572 accuracy 0.4838\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss 0.00479828 accuracy 0.4914\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss 0.00417669 accuracy 0.4862\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss 0.00691336 accuracy 0.4786\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss 0.0121988 accuracy 0.4866\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss 0.00716738 accuracy 0.4854\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss 0.00870641 accuracy 0.4856\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss 0.0116282 accuracy 0.4862\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss 0.0107359 accuracy 0.4852\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss 0.0218972 accuracy 0.4884\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss 0.00371531 accuracy 0.4912\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss 0.00322179 accuracy 0.487\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss 0.00524124 accuracy 0.4846\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss 0.00311627 accuracy 0.4894\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss 0.00475381 accuracy 0.4874\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss 0.00395854 accuracy 0.4766\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss 0.00382058 accuracy 0.4812\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss 0.00499893 accuracy 0.4858\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss 0.00278075 accuracy 0.492\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss 0.00207806 accuracy 0.4792\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss 0.00546978 accuracy 0.4794\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss 0.00918514 accuracy 0.482\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss 0.00289197 accuracy 0.4822\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss 0.0233577 accuracy 0.4804\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss 0.00174929 accuracy 0.476\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.23248 accuracy 0.1896\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss 2.07558 accuracy 0.258\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss 1.79948 accuracy 0.2788\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss 1.84939 accuracy 0.3224\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss 1.85727 accuracy 0.355\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 1.94867 accuracy 0.387\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss 1.76803 accuracy 0.3982\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss 1.38064 accuracy 0.4076\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss 1.55981 accuracy 0.4296\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss 1.65893 accuracy 0.4372\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 1.78931 accuracy 0.4384\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss 1.65028 accuracy 0.4498\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss 1.26472 accuracy 0.4532\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss 1.45902 accuracy 0.469\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss 1.50673 accuracy 0.4752\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 1.61947 accuracy 0.4742\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss 1.46244 accuracy 0.4768\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss 1.12269 accuracy 0.4814\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss 1.3391 accuracy 0.5006\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss 1.36099 accuracy 0.5022\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.50769 accuracy 0.4912\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss 1.34112 accuracy 0.495\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss 1.00403 accuracy 0.5026\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss 1.27985 accuracy 0.5184\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss 1.29339 accuracy 0.5104\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.35683 accuracy 0.5128\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss 1.21725 accuracy 0.5142\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss 0.986482 accuracy 0.5286\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss 1.17609 accuracy 0.5258\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss 1.21056 accuracy 0.5286\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.25916 accuracy 0.535\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss 1.15199 accuracy 0.5394\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss 0.933595 accuracy 0.556\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss 1.10271 accuracy 0.5376\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss 1.12284 accuracy 0.5508\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.12597 accuracy 0.5488\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss 1.02674 accuracy 0.5766\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss 0.847175 accuracy 0.5708\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss 1.02382 accuracy 0.552\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss 1.01702 accuracy 0.5568\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.09172 accuracy 0.5598\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss 0.952924 accuracy 0.5654\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss 0.718338 accuracy 0.594\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss 0.94745 accuracy 0.5668\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss 1.00015 accuracy 0.5742\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.00993 accuracy 0.5846\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss 0.852313 accuracy 0.5778\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss 0.65462 accuracy 0.604\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss 0.825003 accuracy 0.5966\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss 0.942445 accuracy 0.5794\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 0.91349 accuracy 0.6018\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss 0.771784 accuracy 0.5958\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss 0.628873 accuracy 0.606\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss 0.7817 accuracy 0.605\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss 0.85881 accuracy 0.5874\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 0.829202 accuracy 0.6156\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss 0.751264 accuracy 0.6048\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss 0.531542 accuracy 0.615\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss 0.758823 accuracy 0.6196\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss 0.746106 accuracy 0.5864\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 0.748583 accuracy 0.6272\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss 0.75847 accuracy 0.6258\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss 0.473243 accuracy 0.6184\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss 0.683011 accuracy 0.6216\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss 0.648861 accuracy 0.6128\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 0.658553 accuracy 0.631\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss 0.710629 accuracy 0.6308\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss 0.473688 accuracy 0.6364\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss 0.593522 accuracy 0.634\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss 0.553756 accuracy 0.6052\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 0.607474 accuracy 0.6416\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss 0.665723 accuracy 0.639\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss 0.416903 accuracy 0.6506\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss 0.537522 accuracy 0.6474\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss 0.474662 accuracy 0.6144\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 0.60039 accuracy 0.6442\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss 0.571233 accuracy 0.6488\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss 0.396315 accuracy 0.6516\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss 0.491889 accuracy 0.6396\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss 0.391896 accuracy 0.6194\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 0.50211 accuracy 0.6518\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss 0.472208 accuracy 0.6468\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss 0.347603 accuracy 0.6516\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss 0.376903 accuracy 0.656\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss 0.398104 accuracy 0.6316\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 0.457957 accuracy 0.6556\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss 0.4057 accuracy 0.6534\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss 0.361247 accuracy 0.6546\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss 0.375287 accuracy 0.6552\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss 0.306706 accuracy 0.6246\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 0.409654 accuracy 0.659\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss 0.34881 accuracy 0.655\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss 0.287372 accuracy 0.6562\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss 0.348969 accuracy 0.6466\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss 0.318196 accuracy 0.6228\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 0.366388 accuracy 0.6538\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss 0.33352 accuracy 0.6548\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss 0.290982 accuracy 0.653\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss 0.328741 accuracy 0.6406\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss 0.344204 accuracy 0.6202\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 0.331064 accuracy 0.6506\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss 0.326591 accuracy 0.651\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss 0.269835 accuracy 0.646\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss 0.289224 accuracy 0.6488\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss 0.329231 accuracy 0.6246\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.269772 accuracy 0.6502\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss 0.287649 accuracy 0.6518\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss 0.248942 accuracy 0.642\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss 0.273063 accuracy 0.644\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss 0.265164 accuracy 0.6308\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.232822 accuracy 0.6566\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss 0.259392 accuracy 0.6562\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss 0.250645 accuracy 0.6438\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss 0.254404 accuracy 0.6488\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss 0.237765 accuracy 0.6404\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.257683 accuracy 0.6514\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss 0.216818 accuracy 0.6476\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss 0.209303 accuracy 0.6412\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss 0.220056 accuracy 0.635\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss 0.219733 accuracy 0.6414\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.190217 accuracy 0.6478\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss 0.204737 accuracy 0.6468\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss 0.221061 accuracy 0.6364\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss 0.236204 accuracy 0.6208\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss 0.23591 accuracy 0.626\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.236189 accuracy 0.647\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss 0.21872 accuracy 0.6346\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss 0.226373 accuracy 0.6358\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss 0.253018 accuracy 0.6106\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss 0.227149 accuracy 0.629\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.20143 accuracy 0.6354\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss 0.18763 accuracy 0.6552\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss 0.179817 accuracy 0.6456\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss 0.170832 accuracy 0.6374\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss 0.19345 accuracy 0.65\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.20468 accuracy 0.6366\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss 0.234585 accuracy 0.6608\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss 0.155324 accuracy 0.649\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss 0.15799 accuracy 0.6496\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss 0.143869 accuracy 0.6452\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.178622 accuracy 0.6444\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss 0.198295 accuracy 0.6602\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss 0.147636 accuracy 0.6428\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss 0.10857 accuracy 0.6504\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss 0.134973 accuracy 0.6484\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.177375 accuracy 0.6444\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss 0.185101 accuracy 0.6524\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss 0.156917 accuracy 0.649\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss 0.127356 accuracy 0.649\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss 0.12201 accuracy 0.653\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.160282 accuracy 0.6518\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss 0.15562 accuracy 0.6598\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss 0.130181 accuracy 0.6496\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss 0.132183 accuracy 0.6524\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss 0.117127 accuracy 0.6488\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.164558 accuracy 0.6476\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss 0.161039 accuracy 0.6484\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss 0.174781 accuracy 0.6348\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss 0.101851 accuracy 0.6594\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss 0.117297 accuracy 0.6408\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.140541 accuracy 0.6462\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss 0.173164 accuracy 0.6386\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss 0.125398 accuracy 0.6328\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss 0.106773 accuracy 0.6508\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss 0.109265 accuracy 0.6356\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.126319 accuracy 0.6496\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss 0.150351 accuracy 0.6418\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss 0.0960551 accuracy 0.649\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss 0.103165 accuracy 0.6492\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss 0.0905537 accuracy 0.6446\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.141512 accuracy 0.6482\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss 0.15093 accuracy 0.6302\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss 0.0992009 accuracy 0.641\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss 0.0935888 accuracy 0.65\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss 0.0948129 accuracy 0.6354\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.141624 accuracy 0.639\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss 0.1363 accuracy 0.6312\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss 0.116241 accuracy 0.6256\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss 0.098011 accuracy 0.644\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss 0.098244 accuracy 0.6382\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.110575 accuracy 0.6448\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss 0.150498 accuracy 0.6368\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss 0.0845206 accuracy 0.6372\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss 0.109694 accuracy 0.6404\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss 0.0610972 accuracy 0.6392\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.0850046 accuracy 0.65\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss 0.12995 accuracy 0.644\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss 0.0639309 accuracy 0.6392\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss 0.0873115 accuracy 0.641\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss 0.0707706 accuracy 0.647\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.0715425 accuracy 0.649\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss 0.140788 accuracy 0.6404\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss 0.0667025 accuracy 0.6316\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss 0.0970483 accuracy 0.6322\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss 0.0856363 accuracy 0.6418\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.0936006 accuracy 0.6446\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss 0.0999724 accuracy 0.6364\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss 0.0765649 accuracy 0.6408\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss 0.0813398 accuracy 0.6302\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss 0.0923505 accuracy 0.6478\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.0805851 accuracy 0.6422\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss 0.0866416 accuracy 0.6288\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss 0.0570702 accuracy 0.625\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss 0.0854547 accuracy 0.6356\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss 0.0419569 accuracy 0.655\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.0954702 accuracy 0.642\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss 0.100401 accuracy 0.6378\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss 0.0455135 accuracy 0.6402\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss 0.0941494 accuracy 0.6442\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss 0.0556833 accuracy 0.6476\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.0878724 accuracy 0.652\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss 0.0864303 accuracy 0.639\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss 0.0541512 accuracy 0.636\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss 0.0551379 accuracy 0.6476\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss 0.0529547 accuracy 0.6422\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.0677437 accuracy 0.6394\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss 0.0761233 accuracy 0.6342\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss 0.0471165 accuracy 0.6402\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss 0.0483007 accuracy 0.6384\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss 0.0386381 accuracy 0.6464\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.0445853 accuracy 0.6448\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss 0.0667691 accuracy 0.636\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss 0.0361409 accuracy 0.6402\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss 0.0495921 accuracy 0.6416\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss 0.0475133 accuracy 0.6516\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.0558845 accuracy 0.6378\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss 0.0543166 accuracy 0.6354\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss 0.03109 accuracy 0.6292\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss 0.0817304 accuracy 0.6454\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss 0.0475601 accuracy 0.6448\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.0453235 accuracy 0.6382\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss 0.0588709 accuracy 0.637\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss 0.0396053 accuracy 0.6362\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss 0.10795 accuracy 0.6372\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss 0.0644983 accuracy 0.6444\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.0355698 accuracy 0.6442\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss 0.0783414 accuracy 0.6302\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss 0.0447358 accuracy 0.636\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss 0.0851441 accuracy 0.6302\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss 0.0438561 accuracy 0.6376\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.0393191 accuracy 0.647\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss 0.0667271 accuracy 0.622\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss 0.0322055 accuracy 0.6442\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss 0.0692442 accuracy 0.6308\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss 0.0347973 accuracy 0.6408\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.0440702 accuracy 0.6352\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss 0.0495327 accuracy 0.6248\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss 0.0265179 accuracy 0.6408\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss 0.0293056 accuracy 0.6368\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss 0.0346221 accuracy 0.6334\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.0390037 accuracy 0.6386\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss 0.0356515 accuracy 0.6278\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss 0.0470679 accuracy 0.6294\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss 0.0475775 accuracy 0.6262\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss 0.0574483 accuracy 0.6366\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.0278968 accuracy 0.6356\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss 0.0502017 accuracy 0.621\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss 0.0604258 accuracy 0.6286\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss 0.0648157 accuracy 0.63\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss 0.138098 accuracy 0.6268\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.0341836 accuracy 0.6358\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss 0.055595 accuracy 0.6266\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss 0.0534804 accuracy 0.623\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss 0.0488707 accuracy 0.64\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss 0.0815409 accuracy 0.623\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.0453804 accuracy 0.6352\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss 0.0925094 accuracy 0.6328\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss 0.0385137 accuracy 0.6316\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss 0.0632847 accuracy 0.6402\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss 0.0647053 accuracy 0.6102\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.0277879 accuracy 0.6382\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss 0.0822346 accuracy 0.6246\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss 0.0218056 accuracy 0.6294\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss 0.040795 accuracy 0.6442\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss 0.053447 accuracy 0.6258\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.0338542 accuracy 0.6452\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss 0.0629363 accuracy 0.6242\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss 0.0276727 accuracy 0.628\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss 0.118703 accuracy 0.648\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss 0.0310856 accuracy 0.6312\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.0349025 accuracy 0.6434\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss 0.0936018 accuracy 0.6252\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss 0.0226017 accuracy 0.6412\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss 0.0304122 accuracy 0.6424\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss 0.0449672 accuracy 0.6418\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.0198608 accuracy 0.6446\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss 0.0440349 accuracy 0.6224\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss 0.0178475 accuracy 0.634\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss 0.0359367 accuracy 0.6404\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss 0.0361391 accuracy 0.639\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.0129582 accuracy 0.6512\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss 0.051006 accuracy 0.629\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss 0.0289 accuracy 0.6314\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss 0.0756395 accuracy 0.6406\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss 0.0219628 accuracy 0.6462\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.0207604 accuracy 0.645\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss 0.0154249 accuracy 0.6266\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss 0.01822 accuracy 0.6296\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss 0.0314183 accuracy 0.6454\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss 0.0218586 accuracy 0.635\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.0411923 accuracy 0.6448\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss 0.0246058 accuracy 0.631\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss 0.0280426 accuracy 0.6328\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss 0.046266 accuracy 0.6484\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss 0.0399669 accuracy 0.6426\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.0241922 accuracy 0.6408\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss 0.0262986 accuracy 0.6268\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss 0.0357772 accuracy 0.6298\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss 0.030195 accuracy 0.6412\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss 0.0187931 accuracy 0.6456\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.00957957 accuracy 0.6488\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss 0.0243072 accuracy 0.6232\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss 0.0183954 accuracy 0.628\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss 0.0219168 accuracy 0.6456\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss 0.0185138 accuracy 0.6414\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.0346095 accuracy 0.6472\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss 0.0334422 accuracy 0.63\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss 0.0274357 accuracy 0.6282\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss 0.0166339 accuracy 0.6438\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss 0.020197 accuracy 0.6444\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.0308237 accuracy 0.6462\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss 0.0343496 accuracy 0.6284\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss 0.0154595 accuracy 0.6306\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss 0.0235093 accuracy 0.6324\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss 0.0189701 accuracy 0.6442\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.0185258 accuracy 0.6458\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss 0.0199439 accuracy 0.6186\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss 0.0230094 accuracy 0.638\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss 0.0765152 accuracy 0.6456\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss 0.0384564 accuracy 0.6404\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.0170269 accuracy 0.6434\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss 0.0226468 accuracy 0.6248\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss 0.0307943 accuracy 0.6348\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss 0.0411094 accuracy 0.6448\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss 0.0107674 accuracy 0.6432\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.0341729 accuracy 0.643\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss 0.0190741 accuracy 0.6288\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss 0.0186708 accuracy 0.6332\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss 0.0252458 accuracy 0.6434\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss 0.0217176 accuracy 0.6428\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.0144852 accuracy 0.6426\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss 0.0161617 accuracy 0.6218\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss 0.00681645 accuracy 0.6374\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss 0.0289634 accuracy 0.6428\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss 0.0142445 accuracy 0.6472\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.015166 accuracy 0.6392\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss 0.0174088 accuracy 0.619\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss 0.0167049 accuracy 0.6326\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss 0.0271076 accuracy 0.6384\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss 0.0272918 accuracy 0.646\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.0187246 accuracy 0.642\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss 0.0154815 accuracy 0.619\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss 0.0199212 accuracy 0.6288\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss 0.0450807 accuracy 0.637\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss 0.0180264 accuracy 0.644\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.0181826 accuracy 0.638\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss 0.0211384 accuracy 0.607\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss 0.0132186 accuracy 0.6374\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss 0.0325541 accuracy 0.6438\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss 0.0178043 accuracy 0.6478\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.0298519 accuracy 0.641\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss 0.00836063 accuracy 0.6124\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss 0.0100069 accuracy 0.6394\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss 0.0155928 accuracy 0.6414\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss 0.0128737 accuracy 0.6508\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.0172573 accuracy 0.6374\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss 0.013531 accuracy 0.611\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss 0.0210293 accuracy 0.6372\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss 0.00922725 accuracy 0.6416\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss 0.0314418 accuracy 0.644\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.0227263 accuracy 0.6322\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss 0.028914 accuracy 0.601\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss 0.00556079 accuracy 0.6428\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss 0.0258822 accuracy 0.6438\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss 0.0177609 accuracy 0.6352\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.0100373 accuracy 0.6444\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss 0.0226258 accuracy 0.613\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss 0.00623939 accuracy 0.6414\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss 0.0175473 accuracy 0.6434\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss 0.0113943 accuracy 0.639\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.0145932 accuracy 0.6388\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss 0.00714678 accuracy 0.6198\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss 0.00573626 accuracy 0.6424\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss 0.00948761 accuracy 0.6386\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss 0.0200099 accuracy 0.64\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.0128912 accuracy 0.6448\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss 0.0116117 accuracy 0.6168\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss 0.0117708 accuracy 0.6476\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss 0.0152404 accuracy 0.6418\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss 0.0124687 accuracy 0.6306\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.0115995 accuracy 0.6396\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss 0.0212013 accuracy 0.6056\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss 0.012452 accuracy 0.6402\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss 0.0169786 accuracy 0.636\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss 0.0282558 accuracy 0.6364\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.00361694 accuracy 0.6428\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss 0.0200386 accuracy 0.6158\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss 0.00794072 accuracy 0.642\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss 0.0117655 accuracy 0.6406\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss 0.0123393 accuracy 0.636\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.0112067 accuracy 0.6396\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss 0.0104939 accuracy 0.6222\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss 0.00668694 accuracy 0.647\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss 0.026762 accuracy 0.6402\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss 0.015079 accuracy 0.6288\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.0156383 accuracy 0.642\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss 0.0288496 accuracy 0.6238\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss 0.00863208 accuracy 0.639\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss 0.015949 accuracy 0.6384\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss 0.0105671 accuracy 0.6354\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.00629914 accuracy 0.651\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss 0.00799978 accuracy 0.619\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss 0.00393375 accuracy 0.639\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss 0.0115353 accuracy 0.6408\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss 0.0074193 accuracy 0.629\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.0502532 accuracy 0.6506\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss 0.0136227 accuracy 0.6146\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss 0.0039725 accuracy 0.6336\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss 0.0105537 accuracy 0.635\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss 0.0135877 accuracy 0.6288\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.0129994 accuracy 0.6424\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss 0.0106821 accuracy 0.6132\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss 0.00433655 accuracy 0.6426\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss 0.00737072 accuracy 0.6272\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss 0.0120476 accuracy 0.6176\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.0129343 accuracy 0.6412\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss 0.00773773 accuracy 0.6174\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss 0.00597394 accuracy 0.6282\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss 0.0106421 accuracy 0.6302\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss 0.0101438 accuracy 0.6248\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.0114779 accuracy 0.6354\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss 0.00596845 accuracy 0.6188\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss 0.00289935 accuracy 0.632\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss 0.00752347 accuracy 0.6368\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss 0.00648526 accuracy 0.6302\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.00721727 accuracy 0.6382\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss 0.00928366 accuracy 0.6204\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss 0.00487327 accuracy 0.6414\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss 0.0197645 accuracy 0.6324\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss 0.00938591 accuracy 0.6274\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.0041907 accuracy 0.6368\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss 0.00865925 accuracy 0.6226\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss 0.0062888 accuracy 0.6388\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss 0.00493884 accuracy 0.6354\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss 0.0198365 accuracy 0.6304\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.00420371 accuracy 0.6406\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss 0.00272163 accuracy 0.635\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss 0.00453754 accuracy 0.6408\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss 0.00290231 accuracy 0.6376\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss 0.00886302 accuracy 0.6252\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.0240412 accuracy 0.6354\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss 0.00810579 accuracy 0.6262\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss 0.00749417 accuracy 0.6398\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss 0.0051613 accuracy 0.6358\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss 0.0168318 accuracy 0.6182\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.00483877 accuracy 0.6292\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss 0.00408354 accuracy 0.6382\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss 0.000893074 accuracy 0.6362\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss 0.0037306 accuracy 0.6326\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss 0.00558611 accuracy 0.6296\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.0180506 accuracy 0.6298\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss 0.00550902 accuracy 0.6346\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss 0.00425952 accuracy 0.6418\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss 0.00589682 accuracy 0.6398\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss 0.00596713 accuracy 0.6276\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.00537095 accuracy 0.6374\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss 0.00148159 accuracy 0.6374\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss 0.00745228 accuracy 0.6416\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss 0.0209341 accuracy 0.6354\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss 0.0108971 accuracy 0.6308\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.00601465 accuracy 0.6262\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss 0.0130545 accuracy 0.6404\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss 0.000812993 accuracy 0.6438\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss 0.00242852 accuracy 0.6402\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss 0.0108024 accuracy 0.6366\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.005584 accuracy 0.6372\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss 0.00953629 accuracy 0.6388\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss 0.0018973 accuracy 0.6442\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss 0.0132111 accuracy 0.634\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss 0.00748669 accuracy 0.6348\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.00996443 accuracy 0.6282\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss 0.00990454 accuracy 0.6398\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss 0.0030202 accuracy 0.6402\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss 0.0064123 accuracy 0.6424\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss 0.0412769 accuracy 0.6322\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.0128848 accuracy 0.6324\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss 0.0184477 accuracy 0.6426\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss 0.0044198 accuracy 0.6418\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss 0.0102909 accuracy 0.6384\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss 0.0102935 accuracy 0.6314\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.0118798 accuracy 0.642\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss 0.00548141 accuracy 0.6294\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss 0.000769941 accuracy 0.6402\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss 0.00255175 accuracy 0.6326\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss 0.0044427 accuracy 0.6268\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.0108347 accuracy 0.6282\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss 0.0051516 accuracy 0.6468\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss 0.00275408 accuracy 0.6396\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss 0.00216643 accuracy 0.6414\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss 0.0015454 accuracy 0.635\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 0.00294709 accuracy 0.6366\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss 0.0035267 accuracy 0.6382\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss 0.00184011 accuracy 0.6334\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss 0.00322945 accuracy 0.6344\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss 0.00463108 accuracy 0.6348\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 0.00931554 accuracy 0.6194\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss 0.00464455 accuracy 0.6392\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss 0.00304822 accuracy 0.6428\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss 0.00385164 accuracy 0.6402\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss 0.00386926 accuracy 0.6322\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 0.00607175 accuracy 0.6268\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss 0.0093628 accuracy 0.6378\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss 0.0041287 accuracy 0.6458\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss 0.00658575 accuracy 0.6366\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss 0.0008941 accuracy 0.6356\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 0.00511642 accuracy 0.6288\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss 0.0063606 accuracy 0.6434\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss 0.00100579 accuracy 0.647\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss 0.00972497 accuracy 0.6372\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss 0.00520913 accuracy 0.6412\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 0.000820149 accuracy 0.6298\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss 0.0036386 accuracy 0.6454\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss 0.00374731 accuracy 0.6428\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss 0.00744452 accuracy 0.6334\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss 0.00450401 accuracy 0.6366\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 0.038431 accuracy 0.6356\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss 0.00666487 accuracy 0.6336\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss 0.00225933 accuracy 0.635\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss 0.00443608 accuracy 0.6352\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss 0.0256736 accuracy 0.6378\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 0.00527933 accuracy 0.6276\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss 0.0111268 accuracy 0.6376\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss 0.00268197 accuracy 0.6342\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss 0.00242355 accuracy 0.6406\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss 0.00246719 accuracy 0.637\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 0.0113093 accuracy 0.6352\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss 0.00497407 accuracy 0.638\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss 0.00412007 accuracy 0.6334\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss 0.00546097 accuracy 0.641\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss 0.00790141 accuracy 0.64\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 0.0296687 accuracy 0.6344\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss 0.0101486 accuracy 0.639\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss 0.00718386 accuracy 0.6392\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss 0.00279061 accuracy 0.6312\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss 0.002276 accuracy 0.6386\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 0.00226368 accuracy 0.6384\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss 0.00680958 accuracy 0.6384\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss 0.0152307 accuracy 0.6366\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss 0.0038613 accuracy 0.6378\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss 0.00661112 accuracy 0.6324\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 0.0233605 accuracy 0.6348\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss 0.0109421 accuracy 0.6376\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss 0.00444789 accuracy 0.6386\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss 0.00150695 accuracy 0.636\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss 0.00550903 accuracy 0.638\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 0.00863881 accuracy 0.6402\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss 0.00791474 accuracy 0.638\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss 0.00169548 accuracy 0.6456\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss 0.00805224 accuracy 0.64\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss 0.00861487 accuracy 0.6414\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 0.000810869 accuracy 0.6308\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss 0.00463907 accuracy 0.6404\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss 0.00381854 accuracy 0.6458\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss 0.00376029 accuracy 0.6392\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss 0.000527686 accuracy 0.6372\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 0.00958395 accuracy 0.6384\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss 0.00972874 accuracy 0.6452\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss 0.00419827 accuracy 0.6464\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss 0.00356681 accuracy 0.6386\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss 0.00546343 accuracy 0.6326\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 0.00589094 accuracy 0.6398\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss 0.00245018 accuracy 0.6376\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss 0.00118019 accuracy 0.6374\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss 0.00274223 accuracy 0.6384\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss 0.00431388 accuracy 0.638\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 0.00371944 accuracy 0.6368\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss 0.00173424 accuracy 0.6474\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss 0.00174253 accuracy 0.6356\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss 0.00306178 accuracy 0.6352\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss 0.00442122 accuracy 0.631\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 0.0204336 accuracy 0.6336\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss 0.0056463 accuracy 0.6422\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss 0.0024742 accuracy 0.6396\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss 0.0103885 accuracy 0.6292\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss 0.00162437 accuracy 0.6298\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 0.00551557 accuracy 0.638\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss 0.00778324 accuracy 0.6414\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss 0.000942802 accuracy 0.6364\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss 0.00484533 accuracy 0.6424\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss 0.00728435 accuracy 0.6368\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 0.00701022 accuracy 0.6366\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss 0.00527586 accuracy 0.6362\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss 0.000732233 accuracy 0.638\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss 0.00839669 accuracy 0.6356\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss 0.00629446 accuracy 0.6286\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 0.00320535 accuracy 0.6366\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss 0.0151206 accuracy 0.6386\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss 0.00105896 accuracy 0.6382\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss 0.00264904 accuracy 0.6324\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss 0.0027821 accuracy 0.6368\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 0.0018718 accuracy 0.636\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss 0.00632025 accuracy 0.6402\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss 0.00155831 accuracy 0.6398\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss 0.00186261 accuracy 0.6348\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss 0.00468353 accuracy 0.6326\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 0.00442741 accuracy 0.6376\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss 0.00271405 accuracy 0.6426\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss 0.00236978 accuracy 0.6346\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss 0.00154651 accuracy 0.6346\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss 0.00326793 accuracy 0.626\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 0.00171196 accuracy 0.6272\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss 0.000923314 accuracy 0.6404\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss 0.00386155 accuracy 0.6356\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss 0.00176048 accuracy 0.6354\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss 0.00314815 accuracy 0.637\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 0.00224587 accuracy 0.6344\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss 0.00274257 accuracy 0.6324\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss 0.000863915 accuracy 0.6356\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss 0.00230906 accuracy 0.6292\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss 0.00327056 accuracy 0.6326\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 0.00321954 accuracy 0.6274\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss 0.0132486 accuracy 0.6334\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss 0.000534507 accuracy 0.6398\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss 0.00136969 accuracy 0.6356\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss 0.00675979 accuracy 0.6312\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 0.0130114 accuracy 0.628\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss 0.00153384 accuracy 0.6328\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss 0.000598979 accuracy 0.6356\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss 0.00320867 accuracy 0.6274\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss 0.00288576 accuracy 0.6332\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 0.00132251 accuracy 0.631\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss 0.000839757 accuracy 0.6356\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss 0.000937732 accuracy 0.6348\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss 0.00155476 accuracy 0.6292\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss 0.00741152 accuracy 0.6412\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 0.00132675 accuracy 0.6282\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss 0.00481568 accuracy 0.6402\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss 0.000454616 accuracy 0.6404\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss 0.00811011 accuracy 0.6262\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss 0.00223352 accuracy 0.6362\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 0.00277019 accuracy 0.6334\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss 0.00281687 accuracy 0.6414\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss 0.00117455 accuracy 0.635\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss 0.00354007 accuracy 0.635\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss 0.000441608 accuracy 0.6404\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 0.00436357 accuracy 0.6294\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss 0.00239354 accuracy 0.6384\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss 0.000583833 accuracy 0.645\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss 0.00467806 accuracy 0.6334\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss 0.000240782 accuracy 0.631\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 0.00308995 accuracy 0.6416\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss 0.00179772 accuracy 0.6414\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss 0.00119881 accuracy 0.6336\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss 0.00136431 accuracy 0.635\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss 0.000952534 accuracy 0.6378\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 0.00032596 accuracy 0.636\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss 0.00112365 accuracy 0.6406\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss 0.000481923 accuracy 0.6484\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss 0.00788916 accuracy 0.6442\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss 0.000651611 accuracy 0.6288\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 0.000833556 accuracy 0.6322\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss 0.000769682 accuracy 0.6316\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss 0.000551299 accuracy 0.6384\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss 0.0112589 accuracy 0.6286\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss 0.00208102 accuracy 0.6368\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 0.0119372 accuracy 0.6318\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss 0.00243191 accuracy 0.6418\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss 0.00154497 accuracy 0.6386\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss 0.00185166 accuracy 0.6276\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss 0.00189194 accuracy 0.634\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 0.00071785 accuracy 0.6388\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss 0.00576439 accuracy 0.6362\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss 0.00173108 accuracy 0.6372\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss 0.00315489 accuracy 0.6296\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss 0.000696786 accuracy 0.6308\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 0.0104177 accuracy 0.6344\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss 0.00258397 accuracy 0.6364\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss 0.0053699 accuracy 0.6456\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss 0.00783804 accuracy 0.6274\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss 0.00179262 accuracy 0.636\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 0.00112876 accuracy 0.6358\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss 0.000406959 accuracy 0.6386\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss 0.00226065 accuracy 0.6422\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss 0.00173722 accuracy 0.6244\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss 0.00239563 accuracy 0.6406\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 0.000661247 accuracy 0.642\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss 0.00107148 accuracy 0.6378\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss 0.00487917 accuracy 0.6294\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss 0.000583679 accuracy 0.6372\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss 0.000622964 accuracy 0.6304\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 0.00138535 accuracy 0.6368\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss 0.0124275 accuracy 0.6364\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss 0.000778947 accuracy 0.6334\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss 0.00409324 accuracy 0.6292\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss 0.0019647 accuracy 0.6274\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 0.000386786 accuracy 0.6416\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss 0.0123187 accuracy 0.6304\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss 0.0010112 accuracy 0.6328\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss 0.00551285 accuracy 0.6294\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss 0.00520628 accuracy 0.638\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 0.00319543 accuracy 0.6386\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss 0.0022351 accuracy 0.639\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss 0.00149485 accuracy 0.6424\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss 0.00393858 accuracy 0.6352\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss 0.0103066 accuracy 0.6374\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 0.00788992 accuracy 0.6452\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss 0.034149 accuracy 0.6386\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss 0.00165395 accuracy 0.6378\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss 0.0035738 accuracy 0.6192\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss 0.00191379 accuracy 0.6348\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 0.00310153 accuracy 0.6412\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss 0.00412447 accuracy 0.6396\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss 0.000214803 accuracy 0.6338\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss 0.00198155 accuracy 0.6342\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss 0.000493749 accuracy 0.6322\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 0.00171847 accuracy 0.6416\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss 0.00124687 accuracy 0.6344\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss 0.00210754 accuracy 0.6332\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss 0.00755044 accuracy 0.6292\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss 0.00163978 accuracy 0.6294\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 0.000491779 accuracy 0.6372\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss 0.00239693 accuracy 0.6374\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss 0.0012226 accuracy 0.6382\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss 0.0097847 accuracy 0.6236\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss 0.000322712 accuracy 0.6322\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.00191345 accuracy 0.6314\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss 0.00507206 accuracy 0.6344\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss 0.00423324 accuracy 0.6362\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss 0.00153014 accuracy 0.6284\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss 0.00211991 accuracy 0.6312\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.000866509 accuracy 0.6288\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss 0.0121509 accuracy 0.6382\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss 0.000531561 accuracy 0.6364\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss 0.00213218 accuracy 0.638\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss 0.00129046 accuracy 0.6312\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.00400607 accuracy 0.6422\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss 0.0041176 accuracy 0.6422\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss 0.000213255 accuracy 0.638\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss 0.004544 accuracy 0.6224\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss 0.00729576 accuracy 0.6368\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.00133168 accuracy 0.6442\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss 0.00290442 accuracy 0.6428\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss 0.000180634 accuracy 0.6444\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss 0.00437059 accuracy 0.6396\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss 0.00178376 accuracy 0.6312\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.0145846 accuracy 0.6406\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss 0.000676837 accuracy 0.6454\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss 0.00244317 accuracy 0.6352\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss 0.000610822 accuracy 0.6318\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss 0.00096826 accuracy 0.6408\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss 0.00191102 accuracy 0.64\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss 0.00623776 accuracy 0.636\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss 0.000110534 accuracy 0.6362\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss 0.00114382 accuracy 0.629\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss 0.000928764 accuracy 0.6374\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss 0.00111974 accuracy 0.6446\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss 0.00156313 accuracy 0.637\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss 0.00213782 accuracy 0.6348\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss 0.00113127 accuracy 0.6354\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss 0.00038096 accuracy 0.6428\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss 0.00083387 accuracy 0.647\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss 0.00499082 accuracy 0.6394\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss 0.000114058 accuracy 0.6398\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss 0.0045473 accuracy 0.6368\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss 0.000163312 accuracy 0.6364\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss 0.00108342 accuracy 0.634\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss 0.00722279 accuracy 0.6408\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss 0.000349741 accuracy 0.6388\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss 0.00269253 accuracy 0.6364\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss 0.000812951 accuracy 0.64\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss 0.000151 accuracy 0.6456\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss 0.00126615 accuracy 0.6414\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss 0.000218703 accuracy 0.633\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss 0.00326788 accuracy 0.6356\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss 0.00052719 accuracy 0.6372\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss 0.00875193 accuracy 0.6354\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss 0.00255443 accuracy 0.645\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss 0.00127104 accuracy 0.6394\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss 0.00620475 accuracy 0.6308\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss 0.00012151 accuracy 0.6336\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss 0.000362354 accuracy 0.638\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss 0.00601573 accuracy 0.6344\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss 0.000503491 accuracy 0.629\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss 0.000813961 accuracy 0.6368\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss 0.000841603 accuracy 0.637\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss 0.000819372 accuracy 0.6306\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss 0.00412435 accuracy 0.646\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss 0.000966648 accuracy 0.6404\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss 0.00610294 accuracy 0.6364\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss 8.52014e-05 accuracy 0.6384\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss 0.00268033 accuracy 0.6448\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss 0.000528107 accuracy 0.635\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss 0.000102405 accuracy 0.6394\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss 0.00272299 accuracy 0.6368\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss 0.0124967 accuracy 0.6414\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss 0.00985512 accuracy 0.6394\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss 0.00439863 accuracy 0.6398\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss 0.000341028 accuracy 0.6378\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss 0.000626983 accuracy 0.6296\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss 0.00921396 accuracy 0.6442\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss 0.00208504 accuracy 0.6386\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss 0.00300817 accuracy 0.6358\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss 0.000119693 accuracy 0.6394\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss 0.00225892 accuracy 0.6362\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss 0.0178637 accuracy 0.6318\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss 0.00155112 accuracy 0.6436\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss 0.00203033 accuracy 0.6406\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss 0.000347711 accuracy 0.6354\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss 0.00683663 accuracy 0.633\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss 0.00191407 accuracy 0.637\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss 0.00104444 accuracy 0.6426\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss 0.000869003 accuracy 0.6354\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss 0.00143614 accuracy 0.6364\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss 0.00091883 accuracy 0.6352\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss 0.00359555 accuracy 0.6418\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss 0.000249599 accuracy 0.641\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss 0.000584179 accuracy 0.6318\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss 0.00584314 accuracy 0.638\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss 0.000495697 accuracy 0.637\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss 0.000552572 accuracy 0.6356\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss 0.00229446 accuracy 0.6428\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss 0.00181039 accuracy 0.6386\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss 0.000206726 accuracy 0.645\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss 0.00444505 accuracy 0.6286\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss 0.00122202 accuracy 0.6392\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss 0.00099861 accuracy 0.6372\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss 0.00201238 accuracy 0.637\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss 0.000519883 accuracy 0.6372\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss 0.0029312 accuracy 0.6376\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss 0.004386 accuracy 0.6334\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss 0.000800537 accuracy 0.6404\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss 9.48335e-05 accuracy 0.6374\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss 0.000422005 accuracy 0.6374\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss 0.00230015 accuracy 0.6308\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss 0.00332792 accuracy 0.6282\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss 0.000983184 accuracy 0.6418\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss 0.00606605 accuracy 0.6326\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss 0.00101244 accuracy 0.64\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss 0.00116132 accuracy 0.6344\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss 0.000530819 accuracy 0.635\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss 0.000174489 accuracy 0.6452\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss 0.0025147 accuracy 0.6428\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss 0.000156946 accuracy 0.6386\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss 0.00201904 accuracy 0.6356\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss 0.00190042 accuracy 0.6366\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss 0.00017255 accuracy 0.6384\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss 0.000801952 accuracy 0.634\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss 0.00125005 accuracy 0.637\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss 0.000228271 accuracy 0.6286\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss 0.00732157 accuracy 0.6364\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss 0.000215479 accuracy 0.639\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss 0.0011318 accuracy 0.6276\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss 0.00251552 accuracy 0.637\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss 0.000635344 accuracy 0.6326\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss 0.000287763 accuracy 0.6362\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss 0.000696907 accuracy 0.6434\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss 0.00158773 accuracy 0.632\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss 0.000510542 accuracy 0.6422\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss 0.00175275 accuracy 0.6388\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss 0.00102216 accuracy 0.6406\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss 0.00014609 accuracy 0.64\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss 0.000508549 accuracy 0.6412\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss 0.00146184 accuracy 0.6372\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss 0.000665476 accuracy 0.6334\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss 0.000455951 accuracy 0.6412\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss 0.000283911 accuracy 0.6444\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss 0.000885044 accuracy 0.6434\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss 0.00405223 accuracy 0.645\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss 0.00165468 accuracy 0.6308\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss 0.00155064 accuracy 0.6476\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss 4.51126e-05 accuracy 0.6396\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss 0.00119443 accuracy 0.6362\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss 0.00758445 accuracy 0.6358\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss 0.000920764 accuracy 0.6214\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss 0.00172998 accuracy 0.6378\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss 0.000618061 accuracy 0.639\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss 0.00210476 accuracy 0.6354\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss 0.000710267 accuracy 0.6418\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss 0.0015085 accuracy 0.6292\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss 0.000784896 accuracy 0.637\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss 0.000845047 accuracy 0.638\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss 0.000472779 accuracy 0.6358\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss 0.000981241 accuracy 0.642\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss 0.000869942 accuracy 0.6338\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss 0.000627362 accuracy 0.6356\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss 0.000328859 accuracy 0.643\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss 0.00107706 accuracy 0.6416\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss 0.000327124 accuracy 0.6418\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss 0.000361282 accuracy 0.6308\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss 0.00405713 accuracy 0.6388\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss 0.000298269 accuracy 0.6408\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss 0.000571724 accuracy 0.6464\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss 0.00101566 accuracy 0.6418\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss 0.000847738 accuracy 0.6436\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss 8.86525e-05 accuracy 0.6414\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss 0.00145202 accuracy 0.6394\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss 0.00293165 accuracy 0.6406\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss 0.000494427 accuracy 0.6416\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss 0.0023015 accuracy 0.6346\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss 0.00033455 accuracy 0.6434\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss 0.000598448 accuracy 0.642\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss 0.00118035 accuracy 0.6444\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss 0.000716187 accuracy 0.6392\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss 0.00828533 accuracy 0.6302\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss 0.000780559 accuracy 0.6444\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss 0.000363773 accuracy 0.6396\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss 0.000766903 accuracy 0.645\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss 0.0010645 accuracy 0.6464\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss 0.000471231 accuracy 0.6426\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss 0.00164113 accuracy 0.6326\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss 0.000109931 accuracy 0.6486\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss 0.000631961 accuracy 0.6472\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss 0.000307492 accuracy 0.6382\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss 0.0018063 accuracy 0.6312\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss 0.000412684 accuracy 0.6426\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss 0.000645905 accuracy 0.6406\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss 0.00118927 accuracy 0.6436\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss 0.00159565 accuracy 0.6432\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss 0.000307246 accuracy 0.6268\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss 0.00100397 accuracy 0.6304\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss 0.00122572 accuracy 0.6468\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss 0.000185392 accuracy 0.6394\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss 5.17961e-05 accuracy 0.6386\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss 0.000976412 accuracy 0.6282\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss 0.00175485 accuracy 0.6382\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss 0.0050636 accuracy 0.6412\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss 6.50501e-05 accuracy 0.6338\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss 0.000100945 accuracy 0.6388\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss 0.000468007 accuracy 0.6298\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss 0.00014798 accuracy 0.637\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss 0.00461448 accuracy 0.6422\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss 0.000938717 accuracy 0.6376\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss 0.00156575 accuracy 0.6386\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss 0.000183204 accuracy 0.631\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss 0.0015642 accuracy 0.641\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss 0.000721052 accuracy 0.641\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss 0.000471013 accuracy 0.633\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss 0.000691096 accuracy 0.6426\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss 0.010516 accuracy 0.6328\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss 0.00132035 accuracy 0.6406\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss 0.00415491 accuracy 0.6402\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss 0.00114047 accuracy 0.6362\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss 0.00048916 accuracy 0.6382\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss 0.000258844 accuracy 0.6336\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss 0.00317326 accuracy 0.6428\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss 0.000523965 accuracy 0.648\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss 0.000461216 accuracy 0.6406\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss 2.19203e-05 accuracy 0.6406\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss 0.000215653 accuracy 0.633\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss 0.00232724 accuracy 0.639\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss 0.000686094 accuracy 0.6442\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss 0.000587316 accuracy 0.643\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss 0.00309064 accuracy 0.6412\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss 0.000133408 accuracy 0.637\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss 0.00702163 accuracy 0.6436\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss 0.000549726 accuracy 0.6408\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss 0.00264388 accuracy 0.6476\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss 0.000348388 accuracy 0.6438\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss 0.00783775 accuracy 0.641\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss 0.000489786 accuracy 0.6348\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss 0.000574932 accuracy 0.6446\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss 0.000373647 accuracy 0.644\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss 0.000751597 accuracy 0.6416\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss 0.00318828 accuracy 0.6356\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss 0.000272669 accuracy 0.6408\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss 0.000299344 accuracy 0.6492\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss 0.00360141 accuracy 0.6354\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss 1.05854e-05 accuracy 0.6408\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss 0.000184227 accuracy 0.6358\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss 0.000108282 accuracy 0.6402\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss 0.000261363 accuracy 0.643\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss 0.000784513 accuracy 0.6436\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss 0.000100199 accuracy 0.6448\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss 0.000121969 accuracy 0.6392\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss 0.000676326 accuracy 0.6434\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss 0.000218744 accuracy 0.644\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss 0.000963676 accuracy 0.6462\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss 0.000183906 accuracy 0.6432\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss 0.00050283 accuracy 0.6382\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss 0.00010578 accuracy 0.6378\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss 0.000911989 accuracy 0.6422\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss 0.000411652 accuracy 0.6382\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss 0.000228485 accuracy 0.642\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss 0.00383817 accuracy 0.6344\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss 0.00298677 accuracy 0.6444\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss 0.000719864 accuracy 0.6512\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss 6.81995e-05 accuracy 0.6456\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss 0.00613632 accuracy 0.6396\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss 0.00236546 accuracy 0.628\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss 0.000904591 accuracy 0.6316\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss 0.000849833 accuracy 0.6432\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss 0.000351764 accuracy 0.6436\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss 3.50246e-05 accuracy 0.6432\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss 0.000522048 accuracy 0.6322\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss 5.16476e-05 accuracy 0.6372\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss 0.00302492 accuracy 0.6422\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss 0.000771027 accuracy 0.6474\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss 0.00380088 accuracy 0.6426\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss 0.000400976 accuracy 0.6368\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss 0.0001831 accuracy 0.6388\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6323646496815286\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b58475470>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
